{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#read the data\ntrain=pd.read_csv(\"../input/Train.csv\")\ntest=pd.read_csv(\"../input/Test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print first few records\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us look at a few test ds obervations as well\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print info-train\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us take copies of train and test datasets \ntrain_orig=train.copy()\ntest_orig=test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us add Item_Outlet_Sales column to test dataset so it is easy to merge train and test where needed\ntest[\"Item_Outlet_Sales\"]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine train and test datasets\ncombi=train.append(test,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us verify a few tailing records\ncombi.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us print the info on combi\ncombi.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us look at item weight feature as it seems to have missing values in it\nimport matplotlib.pyplot as plt\nplt.hist(x=combi.Item_Weight,cumulative=True)\nplt.xlabel(\"Item weight\")\nplt.ylabel(\"Frequency count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us look at skewness and kurtosis values of item weight feature\ncombi.Item_Weight.skew()\ncombi.Item_Weight.kurt()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# it does not sound like we have many outliers in item weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.Item_Weight.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#item weight and outlet size seems to have missing values. fill that with mean or median for item weight.\ncombi.Item_Weight.fillna(combi.Item_Weight.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.Item_Weight.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.Item_Visibility.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at item visibility\nplt.hist(x=combi.Item_Visibility)\nplt.xlabel(\"Item Visibility\")\nplt.ylabel(\"Frequency count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looks like there are some 0(zero) visibility values...could be a mistake or problem with the data feeding. let us identify those\ncombi[combi.Item_Visibility==0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 879 such rows are there where you have zero visibility.\ncombi.Item_Visibility.kurt()\n\n#let us impute mean value into those\nfrom sklearn.preprocessing import Imputer\nimp = Imputer(missing_values=0, strategy=\"mean\", axis=0)\ncombi[\"Item_Visibility\"] = imp.fit_transform(combi[[\"Item_Visibility\"]]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# verify if we still see any missing values (zeros) in item visibility feature\n(combi.Item_Visibility==0).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(combi.Item_Visibility==0).sum()  # no more zeros in visibility column..hurray","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us look at outlet type and size features\ncombi.Outlet_Type.value_counts()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, outlet type column has the following unique values:\n\nSupermarket Type1     \nGrocery Store         \nSupermarket Type3     \nSupermarket Type2     "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us look at outlet size feature\ncombi.Outlet_Size.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looks like there are 4016 missing values in it. let us take a look at some of these observations before we decide on anything further on these."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(combi.Outlet_Type,combi.Outlet_Size,margins=True,dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us take a look at those combinations\ncombi[combi.Outlet_Size.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us fill those missing values with \"Other\"\ncombi.Outlet_Size.fillna(value=\"Other\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.info()   # No more missing values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.Item_Fat_Content.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lools like: LF, Low Fat and low fat are same; Regular and reg are same. let us convert them to \"low fat\" and \"reg\" \n# respectively.\n\nfat_conversion_dict={\"Low Fat\":\"low fat\",\"Regular\":\"reg\",\"LF\":\"low fat\",\"reg\":\"reg\",\"low fat\":\"low fat\"}\ncombi.Item_Fat_Content=combi.Item_Fat_Content.map(fat_conversion_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us take a look at it \ncombi.Item_Fat_Content.value_counts()  # Now, there are only 2 types","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us take a look at object type variables\ncategorical_cols=combi.select_dtypes(include=\"object\").columns  # This includes item identifier\n\nvalue_counts=[combi[col].value_counts() for col in categorical_cols if col not in [\"Item_Identifier\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Go thru each categorical variable and find out how many different values each of them assumes\ncategorical_cols=[]\nfor items in value_counts:\n    print(items.name) # Name of categorical variable\n    categorical_cols.append(items.name)\n    print(items.nunique()) # unique value count for the categorical variable\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us convert all these categorical variables into numeric using one hot coding\ncombi=pd.get_dummies(combi,drop_first=True,columns=categorical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us scale certain numeric features before we do any model building:\nscaled_features=[\"Item_Weight\",\"Item_Visibility\",\"Item_MRP\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\ncombi[scaled_features]=sc.fit_transform(combi[scaled_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi[scaled_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now the numeric features are scaled...let us do a bit more proessing on this:\n# Separate Item_Identifier as it is not needed for model\n# Separate Item_Outlet_Sales into y dataset as it is to be predicted\n\nitem_id=combi[\"Item_Identifier\"]\ny=combi[\"Item_Outlet_Sales\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(item_id.count())\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"combi dataframe has both train and test observations.\nTime to split those."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us divide the combi frame to X and y\norig_train_len=train.shape[0]\norig_test_len=test.shape[0]\nprint(orig_train_len,orig_test_len)\n\ncombi_item_id=combi[\"Item_Identifier\"]  # both train and test\ncombi_y=combi[\"Item_Outlet_Sales\"]      # both train and test \n\nprint(len(combi_item_id))       # both train and test set\nprint(len(combi_y))             # both train and test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build X and y datasets now for model building; ignore original test dataset; ignore item id and target columns\nX=combi.loc[0:orig_train_len-1,combi.columns[~combi.columns.isin(['Item_Identifier','Item_Outlet_Sales'])]]\ny=combi.loc[0:orig_train_len-1,['Item_Outlet_Sales']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let us do k-fold cross validation here with the following regression models:\n1. Linear regression\n2. Ridge regression model\n3. Decision tree regression model\n4. Random Forest regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us start with model selection first here. Since this is a regression problem, import all those relevant models/.\nfrom sklearn.linear_model import LinearRegression,Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us use default parameters and initialize the models accordingly\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('RR', Ridge()))\nmodels.append(('DT', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us get the possible scoring values that can be used in cross validation methods:\nfrom sklearn import metrics\nmetrics.SCORERS.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(y).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K fold cross validation ---perfect way of measuring accuracy of models\nfrom sklearn.model_selection import KFold,cross_val_score\nnames = []\nrmse_scores = []\nfor name, model in models:    \n    kfold = KFold(n_splits=10, random_state=10) \n    rmse_score = np.sqrt(np.abs(cross_val_score(model, X, np.array(y).ravel(), cv=kfold, scoring='neg_mean_squared_error').mean()\n                               ))\n    names.append(name)\n    rmse_scores.append(rmse_score)\nkf_cross_val = pd.DataFrame({'Model Name': names, 'mean RMSE Score': rmse_scores})\nprint(kf_cross_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validation suggests Linear Regression and Ridge regression models result in lowest RMSE values. So, let us go with Linear regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us split this into train and test\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us try linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nlrg=LinearRegression()\nlrg.fit(X_train,y_train)\nlrg_Xtest_predictions=lrg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean sqaured error is:\",mean_squared_error(y_test,lrg_Xtest_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Root Mean sqaured error is:\",np.sqrt(mean_squared_error(y_test,lrg_Xtest_predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print intercept and coefficients of the model\nprint(\"Linear regression model (lrg) Intercept is: \",lrg.intercept_)\nprint(\"Linear regression model (lrg) Coefficients are: \",lrg.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\npredictors=X_train.columns\ncoefficients = pd.Series(np.array(lrg.coef_).ravel(), predictors).sort_values()\ncoefficients.plot(kind='bar', title='Model Coefficients',fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Leave this space blank for submission. We need to do predictions using the above linear regression model on \"original test data\" and submit those predictions into a submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca=PCA(n_components=37)   # all components\npca.fit(X_train)\nprint(\"explained variance ratios: \",pca.explained_variance_ratio_)\ncum_variance=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\nprint(\"cumulative explained variance ratios: \",cum_variance)\nplt.plot(cum_variance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the index of 99% variance\nindex_for_99percent_variance=np.where(cum_variance > 99.0)[0][0]\nindex_for_99percent_variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It looks like we can achieve 99% variance with just 13 components alone.. So, let us build the PCA with just 13 components\npca=PCA(n_components=13)   # only 13 components\npca.fit(X_train)\nX_train_pca=pca.transform(X_train)\nX_test_pca=pca.transform(X_test)\nprint(\"explained variance ratios: \",pca.explained_variance_ratio_)\ncum_variance=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\nprint(\"cumulative explained variance ratios: \",cum_variance)\nplt.plot(cum_variance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us build linear regresion on PCA components (dimensionality reduction happened)\nlrgp=LinearRegression()\nlrgp.fit(X_train_pca,y_train)\nlrgp_Xtest_pca_predictions=lrgp.predict(X_test_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Root Mean sqaured error using PCA is:\",np.sqrt(mean_squared_error(y_test,lrgp_Xtest_pca_predictions)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}